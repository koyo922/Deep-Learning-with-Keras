{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1671)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11149312/11490434 [============================>.] - ETA: 0s60000 train examples, 10000 test examples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "RESHAPED = 28*28\n",
    "\n",
    "x_train, x_test = [a.reshape(-1, RESHAPED).astype('float32')/255 for a in (x_train, x_test) ]\n",
    "y_train, y_test = [np_utils.to_categorical(a, NB_CLASSES) for a in (y_train, y_test)]\n",
    "\n",
    "print('\\n{} train examples, {} test examples'.format(x_train.shape[0], x_test.shape[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 4s - loss: 0.8536 - acc: 0.7994 - val_loss: 0.5090 - val_acc: 0.8770\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.4895 - acc: 0.8758 - val_loss: 0.4178 - val_acc: 0.8934\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.4283 - acc: 0.8857 - val_loss: 0.3821 - val_acc: 0.8978\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3979 - acc: 0.8925 - val_loss: 0.3618 - val_acc: 0.9022\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3786 - acc: 0.8966 - val_loss: 0.3479 - val_acc: 0.9058\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3651 - acc: 0.8993 - val_loss: 0.3383 - val_acc: 0.9083\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3548 - acc: 0.9019 - val_loss: 0.3298 - val_acc: 0.9098\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.3465 - acc: 0.9039 - val_loss: 0.3240 - val_acc: 0.9105\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3399 - acc: 0.9057 - val_loss: 0.3191 - val_acc: 0.9138\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3344 - acc: 0.9069 - val_loss: 0.3149 - val_acc: 0.9135\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3295 - acc: 0.9084 - val_loss: 0.3109 - val_acc: 0.9145\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3252 - acc: 0.9093 - val_loss: 0.3077 - val_acc: 0.9162\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.3216 - acc: 0.9108 - val_loss: 0.3060 - val_acc: 0.9149\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3183 - acc: 0.9111 - val_loss: 0.3027 - val_acc: 0.9161\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3154 - acc: 0.9119 - val_loss: 0.3010 - val_acc: 0.9157\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3127 - acc: 0.9127 - val_loss: 0.2987 - val_acc: 0.9168\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3103 - acc: 0.9135 - val_loss: 0.2971 - val_acc: 0.9178\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3080 - acc: 0.9143 - val_loss: 0.2957 - val_acc: 0.9173\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3060 - acc: 0.9144 - val_loss: 0.2939 - val_acc: 0.9187\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3042 - acc: 0.9149 - val_loss: 0.2927 - val_acc: 0.9179\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3024 - acc: 0.9154 - val_loss: 0.2915 - val_acc: 0.9185\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.3008 - acc: 0.9162 - val_loss: 0.2910 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2993 - acc: 0.9163 - val_loss: 0.2895 - val_acc: 0.9195\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2979 - acc: 0.9167 - val_loss: 0.2884 - val_acc: 0.9192\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2965 - acc: 0.9172 - val_loss: 0.2876 - val_acc: 0.9200\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2953 - acc: 0.9175 - val_loss: 0.2867 - val_acc: 0.9203\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2941 - acc: 0.9178 - val_loss: 0.2860 - val_acc: 0.9202\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2929 - acc: 0.9178 - val_loss: 0.2851 - val_acc: 0.9203\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2918 - acc: 0.9183 - val_loss: 0.2842 - val_acc: 0.9213\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2908 - acc: 0.9187 - val_loss: 0.2836 - val_acc: 0.9207\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2898 - acc: 0.9191 - val_loss: 0.2828 - val_acc: 0.9210\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2889 - acc: 0.9191 - val_loss: 0.2821 - val_acc: 0.9210\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2880 - acc: 0.9198 - val_loss: 0.2817 - val_acc: 0.9212\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2872 - acc: 0.9200 - val_loss: 0.2813 - val_acc: 0.9217\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2864 - acc: 0.9202 - val_loss: 0.2806 - val_acc: 0.9216\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2855 - acc: 0.9202 - val_loss: 0.2800 - val_acc: 0.9208\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2848 - acc: 0.9205 - val_loss: 0.2797 - val_acc: 0.9212\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2840 - acc: 0.9204 - val_loss: 0.2797 - val_acc: 0.9216\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2834 - acc: 0.9208 - val_loss: 0.2789 - val_acc: 0.9225\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2827 - acc: 0.9209 - val_loss: 0.2782 - val_acc: 0.9217\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2820 - acc: 0.9212 - val_loss: 0.2781 - val_acc: 0.9222\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2814 - acc: 0.9213 - val_loss: 0.2775 - val_acc: 0.9226\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2807 - acc: 0.9219 - val_loss: 0.2779 - val_acc: 0.9236\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2802 - acc: 0.9217 - val_loss: 0.2769 - val_acc: 0.9233\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2796 - acc: 0.9221 - val_loss: 0.2764 - val_acc: 0.9226\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2790 - acc: 0.9224 - val_loss: 0.2766 - val_acc: 0.9230\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2785 - acc: 0.9225 - val_loss: 0.2762 - val_acc: 0.9233\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2761 - val_acc: 0.9229\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2775 - acc: 0.9224 - val_loss: 0.2753 - val_acc: 0.9233\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2770 - acc: 0.9229 - val_loss: 0.2751 - val_acc: 0.9223\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2764 - acc: 0.9228 - val_loss: 0.2749 - val_acc: 0.9233\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2759 - acc: 0.9233 - val_loss: 0.2750 - val_acc: 0.9239\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2756 - acc: 0.9229 - val_loss: 0.2743 - val_acc: 0.9236\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2750 - acc: 0.9230 - val_loss: 0.2748 - val_acc: 0.9231\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2747 - acc: 0.9230 - val_loss: 0.2740 - val_acc: 0.9234\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2743 - acc: 0.9235 - val_loss: 0.2742 - val_acc: 0.9240\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2738 - acc: 0.9236 - val_loss: 0.2732 - val_acc: 0.9243\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2734 - acc: 0.9237 - val_loss: 0.2734 - val_acc: 0.9239\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2730 - acc: 0.9242 - val_loss: 0.2731 - val_acc: 0.9239\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2727 - acc: 0.9237 - val_loss: 0.2727 - val_acc: 0.9236\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2723 - acc: 0.9240 - val_loss: 0.2726 - val_acc: 0.9244\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 3s - loss: 0.2720 - acc: 0.9239 - val_loss: 0.2725 - val_acc: 0.9243\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 2s - loss: 0.2716 - acc: 0.9243 - val_loss: 0.2723 - val_acc: 0.9242\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 2s - loss: 0.2711 - acc: 0.9246 - val_loss: 0.2732 - val_acc: 0.9232\n",
      "Epoch 65/200\n",
      " 8992/48000 [====>.........................] - ETA: 1s - loss: 0.2673 - acc: 0.9250"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
